{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T02:42:28.036790Z",
     "start_time": "2020-03-21T02:42:27.978Z"
    }
   },
   "source": [
    "# Analysing JSON Data\n",
    "\n",
    "This example demonstrates how to download and process [JSON](https://en.wikipedia.org/wiki/JSON) datasets using the open source data transformation tool: [Arc](https://arc.tripl.ai/). In this example the data is downloaded directly from the source using an Arc [HTTPExtract](https://arc.tripl.ai/extract/#httpextract) stage but the data could equally have been downloaded prior to the job.\n",
    "\n",
    "This example aims to show how to:\n",
    "\n",
    "1. source data using the [HTTPExtract](https://arc.tripl.ai/extract/#httpextract) stage.\n",
    "1. parse a [JSON](https://en.wikipedia.org/wiki/JSON) response using the [JSONExtract](https://arc.tripl.ai/extract/#jsonextract) stage.\n",
    "1. process the nested result into a standard tabular representation using the [SQLTransform](https://arc.tripl.ai/transform/#sqltransform) stage with the inline `%sql` 'magic' functionality\n",
    "1. ensure data quality and load assurance needs are met using the [SQLValidate](https://arc.tripl.ai/validate/#sqlvalidate) stage.\n",
    "1. write out the data so it can be consumed by other people or jobs.\n",
    "\n",
    "This example is possible due to data provided by the excellent [Australian Bureau of Meteorology](http://www.bom.gov.au)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables\n",
    "\n",
    "To make this process reusable we first define a variable which we can use to dynamically replace the weather station identifier at execution time with a different station. This makes this job reusable for all [Australian Bureau of Meteorology](http://www.bom.gov.au) weather stations.\n",
    "\n",
    "To set a variable to use when developing logic it can be set with the `%env` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env\n",
    "WMO_STATION_ID=94768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T02:51:54.382130Z",
     "start_time": "2020-03-21T02:51:54.348Z"
    }
   },
   "source": [
    "### Download the weather data\n",
    "\n",
    "This step uses the [HTTPExtract](https://arc.tripl.ai/extract/#httpextract) stage to directly download the data at `http://www.bom.gov.au/fwo/IDN60901/IDN60901.94768.json` and makes that dataset available with the alias `weather_raw` (defined by `outputView`). The reponse is the raw response data to allow you to choose how to process the data.\n",
    "\n",
    "Here we are using the `WMO_STATION_ID` variable and [string interpolation](https://en.wikipedia.org/wiki/String_interpolation) to call the endpoint based on the provided station identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"type\": \"HTTPExtract\",\n",
    "  \"name\": \"download weather data\",\n",
    "  \"environments\": [\n",
    "    \"production\",\n",
    "    \"test\"\n",
    "  ],\n",
    "  \"inputURI\": \"http://www.bom.gov.au/fwo/IDN60901/IDN60901.\"${WMO_STATION_ID}\".json\",\n",
    "  \"outputView\": \"weather_raw\",\n",
    "  \"persist\": true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the response\n",
    "\n",
    "This step uses the [JSONExtract](https://arc.tripl.ai/extract/#jsonextract) stage to parse data in the `body` field (defined by `inputField`) of the incoming `weather_raw` dataset (defined by `inputView`) and makes that dataset available with the alias `weather_nested` (defined by `outputView`).\n",
    "\n",
    "If the `IDN60901.94768.json` file had been downloaded prior to this job (instead of being directly downloaded using the [HTTPExtract](https://arc.tripl.ai/extract/#httpextract) stage) then `inputView` and `inputField` would be replaced with an `inputURI` pointing to that file or multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"type\": \"JSONExtract\",\n",
    "  \"name\": \"parse weather data http response\",\n",
    "  \"environments\": [\n",
    "    \"production\",\n",
    "    \"test\"\n",
    "  ],\n",
    "  \"inputView\": \"weather_raw\",\n",
    "  \"inputField\": \"body\",\n",
    "  \"outputView\": \"weather_nested\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the schema\n",
    "\n",
    "Arc runs on top of [Apache Spark](https://spark.apache.org/) which supports advanced data types such as nested objects inside arrays like returned in the `IDN60901.94768.json` [JSON](https://en.wikipedia.org/wiki/JSON) dataset in addition to the standard `string`, `float` and `date` data types. \n",
    "\n",
    "To see the schema and help write queries to extract the data the `%printschema` magic can be used to show the layout of the data within the parsed [JSON](https://en.wikipedia.org/wiki/JSON) response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%printschema\n",
    "weather_nested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode the data\n",
    "\n",
    "[Arc](https://arc.tripl.ai/) aims to help business users safely and independently build their own data processing jobs by removing the multiple translations traditionally required when having to hand off the work to developers or other 'go-between' roles.\n",
    "\n",
    "To do this we need a way of those users expressing business intent for which we employ [SQL](https://en.wikipedia.org/wiki/SQL) as a dialect (which is why we say that [Arc](https://arc.tripl.ai/) is 'SQL First'). We have found that SQL supports most standard data transformation requirements, is relatively easy to learn and easy to hire for.\n",
    "\n",
    "The statement below demonstrates three key operations for processing the `IDN60901.94768.json` nested dataset:\n",
    "\n",
    "1. use of the [POSEXPLODE](https://spark.apache.org/docs/latest/api/sql/index.html#posexplode) SQL function to separates the elements of the `observations.data` into multiple rows. [POSEXPLODE](https://spark.apache.org/docs/latest/api/sql/index.html#posexplode) also returns the `position` of the data (i.e. the index) in the array which can be useful if the position in the array is important. There is also a most simplistic [EXPLODE](https://spark.apache.org/docs/latest/api/sql/index.html#explode) function which does not return the position.\n",
    "1. use of a subquery to turn the data returned by [POSEXPLODE](https://spark.apache.org/docs/latest/api/sql/index.html#posexplode) into a normal tabular representation by selecting the required fields. If desired the use of `observation.*` would also work instead of selecting individual fields.\n",
    "1. parsing the `aifstime_utc` field into a [UTC](https://en.wikipedia.org/wiki/Coordinated_Universal_Time) timestamp which can then be safely used to order the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql name=\"calculate weather\" outputView=weather environments=production,test persist=true\n",
    "SELECT\n",
    "  -- convert aifstime_utc to a timestamp object\n",
    "  TIMESTAMP(\n",
    "    CONCAT(\n",
    "      -- date\n",
    "      SUBSTR(observation.aifstime_utc, 0, 4),'-', SUBSTR(observation.aifstime_utc, 5, 2),'-', SUBSTR(observation.aifstime_utc, 7, 2)\n",
    "      ,' ',\n",
    "      -- time\n",
    "      SUBSTR(observation.aifstime_utc, 9, 2),':', SUBSTR(observation.aifstime_utc, 11, 2),':', SUBSTR(observation.aifstime_utc, 13, 2)\n",
    "    ) \n",
    "  ) AS timestamp\n",
    "  ,observation.air_temp\n",
    "  ,observation.apparent_t\n",
    "  ,observation.delta_t\n",
    "  ,observation.dewpt\n",
    "  ,observation.press\n",
    "  ,observation.wind_spd_kmh\n",
    "  ,observation.history_product\n",
    "  ,observation.wmo\n",
    "  ,header.refresh_message\n",
    "  ,_index\n",
    "FROM (\n",
    "  SELECT\n",
    "    POSEXPLODE(observations.data) AS (_index, observation)\n",
    "  FROM weather_nested\n",
    ") observations\n",
    "\n",
    "CROSS JOIN\n",
    "\n",
    "(\n",
    "  SELECT\n",
    "    EXPLODE(observations.header) AS header\n",
    "  FROM weather_nested\n",
    ") header  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the data / load assurance\n",
    "\n",
    "Before using the data it is a good idea to ensure that certain data quality guarantees can be provided to downstream consumers of the data. To do this we once again use an [Arc](https://arc.tripl.ai/) 'SQL First' approach to define data quality rules using a [SQLValidate](https://arc.tripl.ai/validate/#sqlvalidate) stage. When building [Arc](https://arc.tripl.ai/) we could have tried to impose a set of standardised business rules but inevitably someone would have a case which was not covered but can be met using a SQL statement.\n",
    "\n",
    "The statement below demonstrates three key operations for our processed the `IDN60901.94768.json` flattened dataset:\n",
    "\n",
    "1. to apply individual rules a good method is to use case statements. For example if we want to ensure that all the timestamp values are populated we can write a statement to find any missing values like: `CASE WHEN timestamp IS NULL THEN 1 ELSE 0 END AS timestamp_null`. `CASE` statements allow very detailed business rules to be defined. Many of these rules can be quickly added for different conditions you care about.\n",
    "1. once the individual rules have been applied we need to define what conditions need to be met for the data to be considered to have met data quality guarantees (and return a `TRUE`/`FALSE` response). In this case we are asserting that the record set must meet these condtions for this stage to be successful:\n",
    "  - has 144 rows (3 days of 30 minute interval data)  - this is Load Assurance to ensure all records have been received and processed.\n",
    "  - AND the `SUM` of the `timestamp_null` rule must equal `0` - this is a Data Quality 'completentess' guarantee that can be provided to consumers of the data\n",
    "  - AND the `SUM` of the `air_temp_null` rule must also equal `0` - this is a Data Quality 'completentess' guarantee that can be provided to consumers of the data\n",
    "  \n",
    "1. additionally we can return a message that is added to the logs. in this case we are returning a [JSON](https://en.wikipedia.org/wiki/JSON) formatted string which looks like `{\"count\":144,\"timestamp_null\":0,\"air_temp_null\":0}`. This is very useful when monitoring this job in when it is operational as we can track metrics or set up alerts when certain conditions occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sqlvalidate name=\"validate dataset\" environments=production,test\n",
    "SELECT\n",
    "  COUNT(*) = 144 AND SUM(timestamp_null) = 0 AND SUM(air_temp_null) = 0 AS valid\n",
    "  ,TO_JSON(\n",
    "      NAMED_STRUCT(\n",
    "        'count', COUNT(*),\n",
    "        'timestamp_null', SUM(timestamp_null),\n",
    "        'air_temp_null', SUM(air_temp_null)\n",
    "      )\n",
    "  ) AS message\n",
    "FROM (\n",
    "  SELECT\n",
    "    CASE\n",
    "      WHEN timestamp IS NULL THEN 1\n",
    "      ELSE 0\n",
    "    END AS timestamp_null\n",
    "    ,CASE\n",
    "      WHEN air_temp IS NULL THEN 1\n",
    "      ELSE 0\n",
    "    END AS air_temp_null\n",
    "  FROM weather\n",
    ") input_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:56:45.136183Z",
     "start_time": "2020-03-21T23:56:45.106Z"
    }
   },
   "source": [
    "### Use the data\n",
    "\n",
    "At this point we can write arbitrary SQL against the `weather` dataset as a standard [SQL](https://en.wikipedia.org/wiki/SQL) table using the `%sql` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- find records where the difference between apparent temperature (feels like) and air temperature correlates with wind speed\n",
    "SELECT \n",
    "  air_temp \n",
    "  ,apparent_t\n",
    "  ,air_temp - apparent_t AS difference_t\n",
    "  ,wind_spd_kmh\n",
    "FROM weather\n",
    "WHERE air_temp > apparent_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the data\n",
    "\n",
    "As we are confident our data is of good quality (due to passing the [SQLValidate](https://arc.tripl.ai/validate/#sqlvalidate) stage) we can export the data so it can be safely consumed by other people and jobs. \n",
    "\n",
    "To do so we can use the [ParquetLoad](https://arc.tripl.ai/load/#parquetload) stage to write the data out to a [Parquet](https://parquet.apache.org/) format which can then can be easily reloaded without losing any data precision using a [ParquetExtract](https://arc.tripl.ai/extract/#parquetextract) stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"type\": \"ParquetLoad\",\n",
    "  \"name\": \"write out flattened weather dataset\",\n",
    "  \"environments\": [\"production\", \"test\"],\n",
    "  \"inputView\": \"weather\",\n",
    "  \"outputURI\": \"/home/jovyan/examples/weather/output/\"${WMO_STATION_ID}\"/weather.parquet\",\n",
    "  \"saveMode\": \"Append\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Job\n",
    "\n",
    "Now that a job has been built in this notebook it is possible to execute it using the [Arc](https://arc.tripl.ai/) Docker image. Notice that we have defined the `WMO_STATION_ID` parameter using an environment variable which can be easily changed to a different station. This could be scheduled to run periodically to retrieve this data.\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "--rm \\\n",
    "-v $(pwd)/examples:/home/jovyan/examples:Z \\\n",
    "-e \"ETL_CONF_ENV=production\" \\\n",
    "-e \"WMO_STATION_ID=94768\" \\\n",
    "-p 4040:4040 \\\n",
    "triplai/arc:arc_2.8.0_spark_2.4.5_scala_2.12_hadoop_2.9.2_1.0.0 \\\n",
    "bin/spark-submit \\\n",
    "--master local[*] \\\n",
    "--driver-memory 4g \\\n",
    "--class ai.tripl.arc.ARC \\\n",
    "/opt/spark/jars/arc.jar \\\n",
    "--etl.config.uri=file:///home/jovyan/examples/weather/ProcessJSON.ipynb\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arc",
   "language": "javascript",
   "name": "arc"
  },
  "language_info": {
   "file_extension": "arc",
   "mimetype": "text/arc",
   "name": "arc",
   "nbconvert_exporter": "text",
   "version": "2.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
